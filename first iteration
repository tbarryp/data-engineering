# Install spark on RedHat 8.4
# Update the system
sudo dnf update -y

# Install java
sudo dnf install default-jdk -y
# Verify java was installed
sudo java -version

# Install scala
sudo dnf install scala -y
sudo scala -version

# Install git
# Verify if git is installed
sudo git --version

# If git not installed
sudo dnf install git -y
# Verify git was installed
sudo git --version

# Install spark
# Download spark
sudo wget https://downloads.apache.org/spark/spark-3.0.1/spark-3.0.1-bin-hadoop2.7.tgz

# extract the saved archive using the tar command:
sudo tar xvf spark-*

# Move the unpacked directory spark-3.0.1-bin-hadoop2.7 to the opt/spark directory
sudo mv spark-3.0.1-bin-hadoop2.7 /opt/spark

# Configure the spark environment
echo "export SPARK_HOME=/opt/spark" >> ~/.bash_profile
echo "export PATH=$PATH:$SPARK_HOME/bin:$SPARK_HOME/sbin" >> ~/.bash_profile
echo "export PYSPARK_PYTHON=/usr/bin/python3" >> ~/.bash_profile

# Add the paths to the profile
source ~/.bash_profile

# Verify that spark was successful using pyspark
pyspark

# Exit
exit()

# Install parquet-tools and parquet-metadata
pip3 install parquet-tools
pip3 install parquet-metadata

# Verify that the installation was successful
parquet-metadata

# Collect the data and name it dataset.csv
wget https://davidmegginson.github.io/ourairports-data/airports.csv -O dataset.csv

# Verify that the data was downloaded and show the 5 first rows:
head -n5 dataset.csv

# Run pyspark
pyspark

# Read the dataset
dataset = spark.read.option("header","true").option("inferSchema","true").csv("dataset.csv")

# Verify that the dataset was successful
dataset.show()

# Copy the dataset in parquet format in 5 partitions under /root
dataset.repartition(5).write.parquet("/root/dataset_parquet")

# Verify the file by using parquet-tools package
ls /root/dataset_parquet

# Copy and replace <filename> with the first .parquet file
parquet-tools show /root/parquet_dataset/<filename>

exit()
